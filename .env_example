# .env_example - Configuration for the application
# Copy this file to .env and modify as needed.
# Pydantic-settings will load values from .env automatically.

# Application settings
LOG_LEVEL="INFO"
# HF_TOKEN="" # Optional Hugging Face token for gated models or private models

# Default model configurations
DEFAULT_AUDIO_MODEL_ID="openai/whisper-small"
DEFAULT_IMAGE_MODEL_ID="llava-hf/llava-v1.6-mistral-7b-hf"

# Device and DType settings
# DEFAULT_DEVICE can be "auto", "cuda", "cpu", "mps", etc.
DEFAULT_DEVICE="auto"
# DEFAULT_TORCH_DTYPE_STR can be "auto", "bfloat16", "float16", "float32"
DEFAULT_TORCH_DTYPE_STR="auto"

# Image model specific settings (primarily for Hugging Face models)
IMAGE_MODEL_MAX_NEW_TOKENS=512

# Quantization settings for Image Models (applied if device is CUDA and load_in_4bit is True)
IMAGE_MODEL_QUANTIZATION_LOAD_IN_4BIT=True
IMAGE_MODEL_QUANTIZATION_BNB_4BIT_QUANT_TYPE="nf4" # Options: "nf4", "fp4"
IMAGE_MODEL_QUANTIZATION_BNB_4BIT_COMPUTE_DTYPE_STR="bfloat16" # e.g., "bfloat16", "float16", "float32"
IMAGE_MODEL_QUANTIZATION_BNB_4BIT_USE_DOUBLE_QUANT=True
IMAGE_MODEL_LOW_CPU_MEM_USAGE=True # For HuggingFace from_pretrained's low_cpu_mem_usage flag

# Ollama fallback settings
OLLAMA_ENABLED=True # Master switch for Ollama integration
OLLAMA_BASE_URL="http://localhost:11434" # Base URL for the Ollama API
OLLAMA_MODEL_FOR_FALLBACK="llava" # Default Ollama model to use for general fallback
OLLAMA_LLAMA3_2_VISION_MODEL="llama3.2-vision" # Specific Ollama model for Llama-3.2-Vision fallback

# Audio model specific settings (can be expanded in settings.py if needed)
# Example: AUDIO_MODEL_CHUNK_LENGTH_S=30
# Example: AUDIO_MODEL_BATCH_SIZE=16
